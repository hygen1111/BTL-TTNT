# -*- coding: utf-8 -*-
"""crnnctcp2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JhtD31Qji10tm84Yip6ZayuxH21KCg13
"""

import os, cv2, torch, pandas as pd, random
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from google.colab import drive

# 1. K·∫øt n·ªëi Drive
drive.mount('/content/gdrive')

# 2. C·∫•u h√¨nh th√¥ng s·ªë
CHARS = "0123456789ABCDEFGHKLMNPRSTUVXYZ"
TRAIN_IMG_DIR = "/content/gdrive/MyDrive/BTL-TTNT/5_CRNN_DATASET/train"
TRAIN_LABEL_CSV = "/content/gdrive/MyDrive/BTL-TTNT/train_labels.csv"
TEST_IMG_DIR = "/content/gdrive/MyDrive/BTL-TTNT/5_CRNN_DATASET/test"
TEST_LABEL_CSV = "/content/gdrive/MyDrive/BTL-TTNT/test_labels.csv"
LOAD_PATH = '/content/gdrive/MyDrive/BTL-TTNT/crnn_v1_acc63.pth'
SAVE_PATH = '/content/gdrive/MyDrive/BTL-TTNT/crnn_pro_final.pth'

def preprocess(img):
    img = cv2.resize(img, (160, 32))
    if len(img.shape) == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = cv2.equalizeHist(img)
    canny = cv2.Canny(img, 50, 150)
    img = cv2.addWeighted(img, 1.5, canny, 0.2, 0)
    return img / 255.0

class PlateDataset(Dataset):
    def __init__(self, img_dir, csv_file, augment=True):
        self.img_dir = img_dir
        self.df = pd.read_csv(csv_file).dropna(subset=['image', 'label'])
        self.augment = augment

    def __len__(self): return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(self.img_dir, str(row['image']))
        img = cv2.imread(img_path)

        if img is None:
            return torch.zeros((1, 32, 160)), torch.tensor([1], dtype=torch.long), 1

        label = str(row['label']).upper()
        if self.augment:
            angle = random.uniform(-5, 5)
            h, w = img.shape[:2]
            M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
            img = cv2.warpAffine(img, M, (w, h))

        img = preprocess(img)
        img = torch.tensor(img).unsqueeze(0).float()
        label_idx = [CHARS.index(c) + 1 for c in label if c in CHARS]
        if len(label_idx) == 0: label_idx = [1]
        return img, torch.tensor(label_idx, dtype=torch.long), len(label_idx)

def collate_fn(batch):
    imgs, labels, lengths = zip(*batch)
    return torch.stack(imgs), torch.cat(labels), torch.tensor(lengths)

import torch.nn.functional as F

class CRNN(nn.Module):
    def __init__(self, nclass, nh=256):
        super(CRNN, self).__init__()
        # CNN tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Kh·ªõp k√™nh v·ªõi b·∫£n c≈©)
        self.cnn = nn.Sequential(
            # L·ªõp 1
            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(True), nn.MaxPool2d(2, 2),
            # L·ªõp 2
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(True), nn.MaxPool2d(2, 2),
            # L·ªõp 3
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),
            # L·ªõp 4
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),
            # L·ªõp 5
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),
            # L·ªõp 6
            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),
            # L·ªõp 7
            nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)
        )
        # RNN h·ªçc chu·ªói
        self.rnn = nn.LSTM(512, nh, num_layers=2, bidirectional=True, batch_first=True, dropout=0.3)
        self.fc = nn.Linear(nh * 2, nclass)

    def forward(self, x):
        x = self.cnn(x)
        x = x.squeeze(2).permute(0, 2, 1) # (B, Seq, Feat)
        x, _ = self.rnn(x)
        return self.fc(x)

def decode_predictions(preds, chars):
    preds = preds.argmax(2).transpose(1, 0).contiguous().cpu().numpy()
    decoded_texts = []
    for p in preds:
        tokens = []
        for i in range(len(p)):
            if p[i] != 0 and (not (i > 0 and p[i] == p[i-1])):
                tokens.append(chars[p[i] - 1])
        decoded_texts.append("".join(tokens))
    return decoded_texts

def evaluate(model, loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for imgs, labels, lengths in loader:
            imgs = imgs.cuda()
            preds = model(imgs).log_softmax(2).permute(1, 0, 2)
            pred_texts = decode_predictions(preds, CHARS)
            targets, curr = [], 0
            for l in lengths:
                targets.append("".join([CHARS[i-1] for i in labels[curr:curr+l]]))
                curr += l
            for p, t in zip(pred_texts, targets):
                if p == t: correct += 1
                total += 1
    return (correct / total) * 100 if total > 0 else 0

# 1. Kh·ªüi t·∫°o
train_loader = DataLoader(PlateDataset(TRAIN_IMG_DIR, TRAIN_LABEL_CSV, augment=True), batch_size=32, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(PlateDataset(TEST_IMG_DIR, TEST_LABEL_CSV, augment=False), batch_size=32, collate_fn=collate_fn)

model = CRNN(nclass=len(CHARS) + 1).cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)
criterion = nn.CTCLoss(blank=0, zero_infinity=True)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)

# 2. LOGIC N·∫†P TR·ªåNG S·ªê TH√îNG MINH
best_acc = 62.98
if os.path.exists(LOAD_PATH):
    print(f" ƒêang kh·ªõp n·ªëi tr·ªçng s·ªë t·ª´ file c≈©...")
    old_dict = torch.load(LOAD_PATH)
    new_dict = model.state_dict()

    # L·ªçc l·∫•y danh s√°ch c√°c tensor t·ª´ file c≈© (ch·ªâ l·∫•y weight v√† bias)
    old_params = list(old_dict.values())
    new_names = list(new_dict.keys())
    new_params = list(new_dict.values())

    idx_old = 0
    match_count = 0
    for i in range(len(new_params)):
        # N·∫øu shape kh·ªõp ho√†n to√†n, n·∫°p v√†o model m·ªõi
        if idx_old < len(old_params):
            if new_params[i].shape == old_params[idx_old].shape:
                new_dict[new_names[i]] = old_params[idx_old]
                idx_old += 1
                match_count += 1
            # N·∫øu l·ªách shape (th∆∞·ªùng l√† BatchNorm), b·ªè qua v√† t√¨m tensor ti·∫øp theo
            elif i + 1 < len(new_params) and new_params[i+1].shape == old_params[idx_old].shape:
                continue

    model.load_state_dict(new_dict)
    print(f" Kh·ªõp th√†nh c√¥ng {match_count} l·ªõp tham s·ªë! B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán n√¢ng cao...")
else:
    print(" Kh√¥ng t√¨m th·∫•y file c≈©, b·∫Øt ƒë·∫ßu train m·ªõi.")
    best_acc = 0.0

# 3. V√≤ng l·∫∑p Hu·∫•n luy·ªán
for epoch in range(150):
    model.train()
    loss_epoch = 0
    for i, (imgs, labels, lengths) in enumerate(train_loader):
        imgs, labels, lengths = imgs.cuda(), labels.cuda(), lengths.cuda()
        preds = model(imgs).log_softmax(2).permute(1, 0, 2)
        input_lengths = torch.full((imgs.size(0),), preds.size(0), dtype=torch.long).cuda()

        loss = criterion(preds, labels, input_lengths, lengths)
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        loss_epoch += loss.item()

        if i % 10 == 0:
            print(f"   Ep {epoch+1} | Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f}", end='\r')

    test_acc = evaluate(model, test_loader)
    scheduler.step()
    print(f"\n Epoch {epoch+1}: Loss {loss_epoch/len(train_loader):.4f} | Acc: {test_acc:.2f}%")

    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), SAVE_PATH)
        print(f" K·ª∂ L·ª§C M·ªöI: {best_acc:.2f}% - ƒê√£ l∆∞u.")

print(f"File c·ªßa b·∫°n ƒëang ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.abspath(SAVE_PATH)}")

import os
file_path = '/content/gdrive/MyDrive/BTL-TTNT/crnn_pro_final.pth'
if os.path.exists(file_path):
    stats = os.stat(file_path)
    print(f"‚úÖ File t·ªìn t·∫°i!")
    print(f"üì¶ Dung l∆∞·ª£ng: {stats.st_size / (1024 * 1024):.2f} MB")
    print(f"üïí C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: {os.path.getmtime(file_path)}")
else:
    print("‚ùå Kh√¥ng t√¨m th·∫•y file! H√£y ki·ªÉm tra l·∫°i k·∫øt n·ªëi Drive.")

try:
    # Kh·ªüi t·∫°o l·∫°i model tr·ªëng
    test_model = CRNN(nclass=len(CHARS) + 1).cuda()
    # N·∫°p tr·ªçng s·ªë t·ª´ Drive v√†o
    state_dict = torch.load('/content/gdrive/MyDrive/BTL-TTNT/crnn_pro_final.pth')
    test_model.load_state_dict(state_dict)
    test_model.eval()
    print("‚úÖ Load model th√†nh c√¥ng! Tr·ªçng s·ªë ho√†n to√†n kh·ªõp v·ªõi ki·∫øn tr√∫c m·∫°ng.")
except Exception as e:
    print(f"‚ùå L·ªói khi load model: {e}")

import torch
import cv2
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# --- 1. H√†m ti·ªÅn x·ª≠ l√Ω chu·∫©n (Copy t·ª´ code train c·ªßa b·∫°n) ---
def preprocess_test(img):
    img = cv2.resize(img, (160, 32))
    if len(img.shape) == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = cv2.equalizeHist(img)
    canny = cv2.Canny(img, 50, 150)
    img = cv2.addWeighted(img, 1.5, canny, 0.2, 0)
    return img / 255.0

# --- 2. H√†m d·ª± ƒëo√°n ch√≠nh x√°c ---
def predict_image(model, img_path, chars):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω
    img = cv2.imread(img_path)
    if img is None:
        return "Kh√¥ng t√¨m th·∫•y ·∫£nh"

    processed_img = preprocess_test(img)
    img_tensor = torch.tensor(processed_img).unsqueeze(0).unsqueeze(0).float().to(device)

    model.eval()
    with torch.no_grad():
        # ƒê·∫ßu ra model l√† [Batch, Time, Class] -> [1, 32, 31]
        preds = model(img_tensor)

        # Chuy·ªÉn v·ªÅ d·∫°ng [Time, Batch, Class] ƒë·ªÉ d√πng logic decode c≈© c·ªßa b·∫°n
        preds_for_decode = preds.permute(1, 0, 2).log_softmax(2)

        # S·ª≠ d·ª•ng ƒë√∫ng h√†m decode_predictions b·∫°n ƒë√£ vi·∫øt trong code train
        decoded_text = decode_predictions(preds_for_decode, chars)[0]

    return decoded_text, img

# --- 3. Th·ª±c hi·ªán Test ---
# Ch·ªçn m·ªôt ·∫£nh b·∫•t k·ª≥ t·ª´ t·∫≠p TEST_IMG_DIR
test_files = [f for f in os.listdir(TEST_IMG_DIR) if f.endswith(('.jpg', '.png', '.jpeg'))]
random_file = random.choice(test_files)
image_path = os.path.join(TEST_IMG_DIR, random_file)

# Kh·ªüi t·∫°o l·∫°i model v√† n·∫°p tr·ªçng s·ªë
model = CRNN(nclass=len(CHARS) + 1).cuda()
model.load_state_dict(torch.load(SAVE_PATH))

# Ch·∫°y d·ª± ƒëo√°n
result, original_img = predict_image(model, image_path, CHARS)

# Hi·ªÉn th·ªã
plt.figure(figsize=(10, 4))
plt.imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))
plt.title(f"D·ª∞ ƒêO√ÅN: {result}", fontsize=20, color='red', fontweight='bold')
plt.axis('off')
plt.show()

print(f"File: {random_file}")
print(f"K·∫øt qu·∫£ vƒÉn b·∫£n: {result}")

PROVINCE_MAP = {
    "11": "Cao B·∫±ng", "12": "L·∫°ng S∆°n", "14": "Qu·∫£ng Ninh", "15": "H·∫£i Ph√≤ng", "16": "H·∫£i Ph√≤ng",
    "17": "H∆∞ng Y√™n", "18": "Ninh B√¨nh", "19": "Ph√∫ Th·ªç", "20": "Th√°i Nguy√™n", "21": "L√†o Cai",
    "22": "Tuy√™n Quang", "23": "Tuy√™n Quang", "24": "L√†o Cai", "25": "Lai Ch√¢u", "26": "S∆°n La",
    "27": "ƒêi·ªán Bi√™n", "28": "Ph√∫ Th·ªç", "29": "H√† N·ªôi", "30": "H√† N·ªôi", "31": "H√† N·ªôi",
    "32": "H√† N·ªôi", "33": "H√† N·ªôi", "34": "H·∫£i Ph√≤ng", "35": "Ninh B√¨nh", "36": "Thanh H√≥a",
    "37": "Ngh·ªá An", "38": "H√† Tƒ©nh", "39": "ƒê·ªìng Nai", "40": "H√† N·ªôi", "41": "TP HCM",
    "43": "ƒê√† N·∫µng", "47": "ƒê·∫Øk L·∫Øk", "48": "L√¢m ƒê·ªìng", "49": "L√¢m ƒê·ªìng", "50": "TP HCM",
    "51": "TP HCM", "52": "TP HCM", "53": "TP HCM", "54": "TP HCM", "55": "TP HCM",
    "56": "TP HCM", "57": "TP HCM", "58": "TP HCM", "59": "TP HCM", "60": "ƒê·ªìng Nai",
    "61": "TP HCM", "62": "T√¢y Ninh", "63": "ƒê·ªìng Th√°p", "64": "Vƒ©nh Long", "65": "C·∫ßn Th∆°",
    "66": "ƒê·ªìng Th√°p", "67": "An Giang", "68": "An Giang", "69": "C√† Mau", "70": "T√¢y Ninh",
    "71": "Vƒ©nh Long", "72": "TP HCM", "73": "Qu·∫£ng Tr·ªã", "74": "Qu·∫£ng Tr·ªã", "75": "Th·ª´a Thi√™n Hu·∫ø",
    "76": "Qu·∫£ng Ng√£i", "77": "Gia Lai", "78": "ƒê·∫Øk L·∫Øk", "79": "Kh√°nh H√≤a", "80": "C·ª•c C·∫£nh s√°t giao th√¥ng",
    "81": "Gia Lai", "82": "Qu·∫£ng Ng√£i", "83": "C·∫ßn Th∆°", "84": "Vƒ©nh Long", "85": "Kh√°nh H√≤a",
    "86": "L√¢m ƒê·ªìng", "88": "Ph√∫ Th·ªç", "89": "H∆∞ng Y√™n", "90": "Ninh B√¨nh", "92": "ƒê√† N·∫µng",
    "93": "ƒê·ªìng Nai", "94": "C√† Mau", "95": "C·∫ßn Th∆°", "97": "Th√°i Nguy√™n", "98": "B·∫Øc Ninh", "99": "B·∫Øc Ninh"
}

def get_province(plate_text):
    if len(plate_text) >= 2:
        prefix = plate_text[:2] # L·∫•y 2 s·ªë ƒë·∫ßu
        return PROVINCE_MAP.get(prefix, "Kh√¥ng x√°c ƒë·ªãnh")
    return "Kh√¥ng x√°c ƒë·ªãnh"

def test_with_province(model, img_path, chars):
    # 1. Ti·ªÅn x·ª≠ l√Ω v√† D·ª± ƒëo√°n (Gi·ªØ nguy√™n logic chu·∫©n c·ªßa b·∫°n)
    img_cv = cv2.imread(img_path)
    if img_cv is None: return

    processed_img = preprocess(img_cv)
    img_tensor = torch.tensor(processed_img).unsqueeze(0).unsqueeze(0).float().cuda()

    model.eval()
    with torch.no_grad():
        preds = model(img_tensor).permute(1, 0, 2).log_softmax(2)
        pred_text = decode_predictions(preds, chars)[0]

    # 2. Tra c·ª©u t·ªânh th√†nh
    province = get_province(pred_text)

    # 3. Hi·ªÉn th·ªã k·∫øt qu·∫£
    plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    plt.title(f"Bi·ªÉn s·ªë: {pred_text}\nT·ªânh: {province}", fontsize=14, color='blue')
    plt.axis('off')
    plt.show()

    print(f"--- K·∫øt qu·∫£: {pred_text} | Thu·ªôc t·ªânh: {province} ---")

# Ch·∫°y th·ª≠ v·ªõi m·ªôt ·∫£nh trong t·∫≠p Test
test_files = [f for f in os.listdir(TEST_IMG_DIR) if f.endswith(('.jpg', '.png'))]
test_with_province(model, os.path.join(TEST_IMG_DIR, random.choice(test_files)), CHARS)